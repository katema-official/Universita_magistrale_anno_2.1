{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1040{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.22000}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\qc\f0\fs22\lang16 Appunti di Heuristic Algorithms\par

\pard\sa200\sl276\slmult1\qj Lezione 1 - 29/09/2022\par
[he wanted us to download the lessons]\par
Heuristics algorithms are not for specific problems. They are general ideas that work widely for any possible problem. Then those ideas are adapted to the particual problem. We'll see how we adapt general ideas to specific problems and then evaluate if the algorithm is good or not. First, some historical ideas. Then, we'll see that "heuristics" are used in many contexts. Then, how to evaluate the performance. Heuristic comes from greek that means "I find", based on the famous story of Archimetes and the golden crown. He wanted to understand if that crown was authentic or not. it was based on the volume, measured putting the crown in a "pool". The concept of heuristics go back to the 4th century a.c. about the ideas that help find proofs for math theorems. These ideas were used in 17th century onward. Heuristics were a collection of general ideas, intuitions.\par
The idea of the heuristic algorithms is that we cannot ensure that we will have a good solution, but most of the time will be a good solution (or the correct/best solution). We can use distributions that tell us how much it is likely that we'll get a good solution. We'll talk about some heuristics for some problems. We consider only Combinatorial optimization. We'll only consider heuristics based on solutions and not models. \line Problems in fact can be classified based on the nature of their solution:\line -Decision problems: true or false.\line -Search problems: the solution is any feasible subsystem (that is, satysfing certain conditions).\line -Optimization problems: The solution is the minimum or maximum value of an objective function defined on the feasible subsystems. Usually, optimization and search are tied together.\line -Counting problems: number of feasible subsystem.\line -Enumeration problems: collection of all feasible subsystems.\par
The solutions in a CO problem is a combination. So, a CO problem can also be considered as a Subset Optimization problem. A combination is: we have n elements and take k of them. \par
Solution based heuristics: heuristics where we use and manipulate subsets of the ground set. \par
Metaheuristic is the common name for heuristic algorithms with randomization and/or memory. \par
5 parts of the course. This lesson and tomorrow one is the first part. We concern on what is a CO and its ground set. Also, is your algorithm good or not? This can be discussed at priori or after executions. We can also give a lower bound on the performances of an algorithm. "My algorithm won't give a solution worse thatn half of the optimum", for example. \par
Let's start with a suervey of problems that are examples. \line 1st: knapsack problem. The ground set is the set of objects to put in the knapsack, because a subset of them is a (possible) solution. From now on, we will distinguish the solutions between Feasible and Unfeasible. \line Problem of three points most distant: a ground set (compact) is the set of all points. Another one (that uses the trick) is the set of all triples. How do we represent the FEASIBLE solution? \{A,B,C\} as a vector is an example. Antoher one is an incidence vector: [1,1,1,0,0,0,0] (1 for the points i count, 0 otherwise). \par
In the knapsack, the objective function is ADDITIVE. This means that in the function we have a sum of all the elements of the solution with a function (?).\par
Flat objective functions work bad with the BPP and the PMSP when the exchange heuristic is used, but the same heuristic is good for the Knapsack.\par
\par
Lezione 2 - 30/09/2022\par
Yesterday, we saw four problems of combinatorial optimization. Some questions were: what is the ground set? There are multiple answers and some ground sets are better than others. Let's also ask ourselves some questions about the combinatorial optimization problem. They are related on the representation of the solutions and such. \line Flat representation: bad for exchange heuristics, since different solutions may have the same value.\par
Max-SAT problem. A logical variable is a variable that can be true or false, can only assume two values. Logical variables can be used in logical functions. \par
...\par
16 -> ground set for SAT: for example, cartesian product of variables and B (0,1). Another one, simpler, is one that takes as subset only the true variables. The objective function is not additive since is not a sum over the elements of the solution. We are summing something that is not related to (x_i, b), but to the conjunction of OR formulas. \par
18 -> Do all the tasks (rows) with minimal cost (value for each column). Ground set: set of columns. A subset of them is a possible solution. The OF is additive.\par
19 -> The feasibility test. Is this a subset or a feasible solution? important question. \line Exchange can create feasible or unfeasible solutions, really depends. in PMSM exchange makes always feasible solutions, in BPP it might not. \par
Set covering: each row must be covered at least once.\par
Set packing: each row must be covered at most once.\par
Set partitioning: each row must be covered EXACTLY once. (we always want to maximise the value of the sum of columns).\line\b Can you find a feasible solution? Is there an easy way to find it?\b0  Exchange heuristics start from a solution. Recombination heuristics start from a population of solutions. In this case, it is hard. \line Set covering -> take all columns (first solution)\line Set packing -> take no column (first solution)\line Set partitioning -> Difficult. Feasibility is NP-complete. One thing we can do is: I'm not sure I can find an element in X, but thanks to a relaxation of the feasible region (X') maybe I can find a feasible solution for the relaxation. \par
So, finding a feasible solution and understanding if a solution is feasible is a main concern. \par
25 -> Graph problems: Vertex Cover Problem. Find a set of vertexes that cover all the edges. An edge is covered if one of the vertex it touches is selected for the solution.\par
35 -> can we find a feasible solution? It depends. Is there a Hamiltonian circuit? This is a strong NP-hard question. But if a complete graph is given, we can take any permutation and it is a possible solution. \par
\par
Lezione 3 - 06/10/2022\par
37 -> This problem (CMSTP) is used for connecting servers. NP-hard to find optimal solution. In linear time (with respect to the set x), we can see if the solution is feasible or not. The obj function is the sum of functions over elements of the solution, so it's additive. Also for exchange heuristics is good, but if we exchange an edge with another we have to re-visit most of the graph to see if it is feasible. \line Here we have vertexes that we want to put in subtrees (BPP ispossib\'f2l?). \par
39 -> (Vertex, Subtree). Also with this ground set we can represent a solution. Checking the feasibility? We just count how many vertexes are in a subtree. But this is ok for not exceeding W. We also have to check if all the nodes of the subtree are conneceted and at least one is connected to r. This is more complex. BUT, if it is a complete graph, it's much easier. So, IN SPECIFIC SUBSET INSTANCES OF THIS PROBLEM, this representation is good (very good). How do we compute the cost anyway? We should have edges, that are not given in the representation. So, in order to compute the objective value of a solution x, we have to resolve the MST problem for each subtree. I take the cheapest tree that contains all the nodes belonging to that representation. (MST = Minimum Spanning Tree). Here, the complexity of computing the objective function is not linear, but is costly (not a lot anyway). \par
44 -> So, which representation is the best? Depends. What am I going to evaluate more times? Feasibility or obj. function? That should be the simpler. One can use both representations! ...but that doubles memory and updates to a solution. Is this a disadvantage? Depends. Sometimes, it is convenient to have two representations. It's true that they must be consistend, but one allows easy computation for feasibility and the other for the objective function. \par
43 -> x it's a feasible solution if each vertex has an ongoing and outgoing edge in x. Also, check if the cycles created don't exceed the maximum capacity. Using the other representation: it's easy to compute the weight of each sub-cycle, and if it is complete as a graph it's easy to find the cycle. But, obj function? Solve the TSP for each sub-cycle (NP-complete :( )\line But when we talked about the Max-SAT, we proposed x = subset of variables made true. Another proposal was x = clausles made true. In this last case, the feasibility is SAT. I mean, you'd have to compute a SAT solution to understand if x is a feasible solution for a SAT problem. That's stupid. \line But in 43, if the graph is complete and W is small, it is true that we need to solve TSP for sub-problems, but the instances are so small that we can do it! But the small TSP, how is computed? Exactly or with another heuristic? eh...\line It has been shown that using heuristic to solve those small TSP can lead to good solutions. \par
Slide 03:\par
A priori and a posteriori analysis. Today we talk about cost, tomorrow about quality. \par
2 -> A problem is made by many cases, that are all similar. I have infinite instances, possibly. For each instance, i have a solution. The problem can be seen as a function! I -> S. The problem is a function that maybe we can compute (depends on the instance), but maybe no. \par
3 -> We can have different algorithms for the same problem. \par
4 -> Cost is time and space. It is possible to reduce the use of time by allowing the use of more space and vice-versa. \par
13 -> What if the capacity of KP is small? Well, in that case, even if I have a lot of items (n), the problem is easy, because in general at most two items can stay in the knapsack. In this case, the complexity is quadratic. \par
circa 16 - Parametrized complexity. We have a parameter that affects the complexity.\par
\par
Lezione 4 - 07/10/2022\par
13 and around -> find the worst instance not only with respect to n, but also with respect to k, another parameter. \par
24 -> after describing the probabilistic model, you have to find the expected value over each of the slices, for each value of n. You can build a probabilistic model based on simulations. \par
25 -> Let's talk about (binary) matrices. For the Set Covering Problem, we had columns that cover rows. How can we give a probabilistic model for those matrices?\line -equiprobability: there are 2^mn matrices. If I assume that each of them has the same probability, i compute the complexity starting from this assumption.\line -uniform probability: each cell is set to 1 with a certain probability\line -fixed density: extract a certain numner of cells with a uniform probability and set them to 1. Dense matrix = fast algorithm, sparse matrix = slow algorithm. In fixed density, we have exactly delta ones. In the equiprobability, the process is still random.\par
26 -> the first case is a particular case of the second. \par
29 -> Sat exhibits phase transitions. OCCHIO: the blu part on the right has the same role of the red one on the left, and viceversa. \par
31 -> In heuristics, the concept of complexity is a bit different than the usual one.\par
lez_04\par
2 -> If efficency means low cost, effectiveness means quality of the solution. So, we need the concepts of closeness and frequency. We can also do a priori and a posteriori analysis. A priori means: I prove that my algorithm also gives a solution that has a certain quality.\par
3 -> There are three definitions for the effectiveness of a heuristic optimization algorithm. f_A(I): value given on instance I by my heuristic algorithm A. I'd like this value to be equal to f*(I), the true optimal value of the instance. \line What if you don't know the optimum? You can construct a lower and upper bound for the optimum. \line (Lower bound = dual bound, upper bound = primal bound). \par
4 -> Up until now, the reasonings were based on experimental analysis (particualr instances). Let's do a thoeretical analysis. \line For the minimum coloring graph problem, the minimum number of colors you have is either the dimension of (the maximum degree) the max clique in the graph or that value +1. \line The last inequality tells us that what our heuristic finds is always bounded by a function of n.\par
\par
Lezione 5 - 13/10/2022\par
We've seen a priori study of heuristic algorithms. In particular, a priori cost and effectiveness. \par
4\'b0 slide\par
14 -> Better approx algos but worse time, there is this trade-off in some algorithms. Not all problems can actualy be approximated. \par
13 -> example: the TSP. Inapproximable = can't prove that your algorithm is alpha times the optimum. \par
5\'b0 slide\par
2 -> empirical evaluation of a heuristic algorithm. The thoeretical analysis is complicated, and a precise answer might not be practical. So, we can follow an experimental approach.\par
3 -> Experimental approach is: bserve the field of your study, then build the model. Then, design experiments to evaluate the model, analyse the results and revise the model based on the results until a satisfactory model. The model could be debunked, in that case you modify it. Or, your experiments prove your system to be correct. \par
4 -> we want to find indicies of efficiency and effectiveness of an algorithm. Then, use this indices to compare different algorithms. In this way, you conlcude that an algo is better than another one. Maybe my algo is good on small instances but good on greater instances.\par
5 -> how do we perform experiments? giving a benchmark of instances. Ideally, it is infinite. practically, it's a subset. What subset? It must be representative of the whole set, relating to structural features. The instances should not be too easy or too hard\par
6 -> the environment influence the result and makes it difficult to reproduce precisely the result. \par
7 -> we sould describe the cost and quality of our algorithms. The only time in which we can say that we prefer an algorithm over another is when it takes not much time and the quality is good. Otherwise, the time or the result must be similar to compare the other feature. \par
8 -> statistical model of algorithm performance? We have the domane of instances, and from it we extract a sample set/benchmark. From it, we randomly extract an instance each time we need one. T_A(I) is the computational time that is a random variable. The relative difference delta is also a random variable. \par
9 -> When i try to compute the relative difference, i usually don't have the optimal value So, we have to estimate delta_A(i). So, we need LOWER BOUND and UPPER BOUND, eccoliiiiiiiiii.\line The lower bound is not greater thatn the optimal, while the upper bound is greater than the optimal. \par
10 -> We'll see a lot of pictures. Le'ts see the Runtime Distribution Diagram. It's a plot that on x axis gives a time, on y gives how likely it is to solve an instance that takes that time. Bascially: given instance i, and fixed time x, y is the probability that i will be solved in that time. \par
11 -> Main features: monotone nondecreasing. It's also not continuous. In zero or before 0 the diagram is equal to zero (you need some time to solve the problem). It's also equal to 1 from a certain point on. \par
13 -> up till now, we assumed that the size was fixed. What happened if we allow time to change? Scaling diagram. x = size of the instance. y = time to solve it. \par
15 -> The Solution Quality Distribution is a diagram of the relative difference. \par
\par
Lezione 6 - 14/10/2022\par
Slide 5\'b0\par
10 -> it's a probability distribution function. \par
14 -> number of elementary operation you perform but also time (?). Depends on the computer you do your runs. \par
15 -> Should we fix the size of the instances or not? Depends. A thing you could do is: consider different sizes, and get profiles of the relative differences. \par
18 -> parametric diagrams. Let's fix the size of the instances and build, for each of them, the SQD diagram. This might show us a trend. As you increase the size of your problem, your algorithm becomes less precise and more heuristic (this is something that can happen). \par
19 -> you can use the SQD to compare the performances of different algorithms. Strict dominance: an algorithm is better than another when the relative difference of the first is better than the relative difference of the second on all instances. Strict dominance implies probabilistic dominance. \par
slide 6\'b0\par
2 -> Compact statistical descriptions. Mean and Variance depend a lot on outliers, unfortunately. The average is used, the variance is less used.\par
3 -> The median divides in two the benchmark. Compute minimum, mazimum, median, etc.. might be a question. Usefulness of this diagram? First: we can observe that this diagram is a simplification of the SQD. Till the minimum, the SQD would be 0. from minimum to lower quartile, the SQD goes up to 25%. Then, from it to the median, goes up to 50%, and so on. From the SQD, you can have the box plot, too. We can use box plot to compare algorithms.\par
4 -> for different algorithms, we don't draw the SQD but the boxplot. The small circles are outliers: they are outside the box. Often happens that an algorithm gets a certain result, but sometimes gets out of the way. Ok mi torna che figoooo. Compare A7 and A8. What is better? A7. Why is it clearly better? because its box is ompletely below the other. This is strict dominance. For probabilistic dominance, we have A2 and A3. It's possible that we don't have strict dominance, but also the contrary is true. Wait, how do we impose probabilistic dominance? That each quarter is better than the previous one. No. Each quartile MUST preceed the corresponing of the other one, ok. But ALSO, it has to preveed the NEXT quartile of the following. \line ok aspetta, ho capito. il secondo del primo deve essere del tutto sotto il primo del secondo.\par
Il terzo del rpimo deve essere meglio del secondo del secondo.\par
il quarto del primo deve essere meglio del terzo del secondo, ok, quando questo \'e8 vero, hai probabilistic dominance. Il rpimo \'e8 probabilistic dominant sull'altro.\par
7 -> Using a randomized algorithm depends on instance, time and seed of the pseudo random number generator. \par
8 -> You can classify algos in at least three ways.\line 1) Complete/exact algorithms. For every instance, it gives the optimum in finite time. The time depends on the size of the instance. Every instance basically has a time after which we are sure that the algorithm gives us the optimum. \par
2) Probabilistically approximately complete. Running the instance for a long time gives us the optimum or not? The probability converges to 1 as the time goes to infinity. SOME meteheuristics don't fall in this cathegory. Is this property interesting? \par
3) Essentially incomplete: finds the optimum for some instances with probability strictly lower that 1 as t goes to infinity. For many other instances, finds the optimum with pro = 1 when t goes to infinity.\par
9 -> alpha complete algorithm is an approximation algorithm. bla bla bla\par
11 -> Fix a certain quality. What happens for a certain time?\par
prendi il grafo di destra, x = 10, y = 30, dotted line: Il 30% delle istanze risolte in maniera ottima vengono risolte in meno di 10 secondi. La linea continua a crescere perch\'e9, se il 30% delle istanze le risolvi in tempo x, le risolvi di sicuro anche in tempo x+c.\par
13 -> Median = 0.5\par
14 -> The idea is that we can compare two algorithms, but what if there is no probabilistic dominance?\par
4 again -> Very small box and large whiskers and viceversa? First, the box is half of the insances (average half). Small box = stable algorithm. \par
14 -> minimization problem. if the difference is always negative, strict dominance. if sometimes it is, sometimes it is not, no guarantee. \par
\par
Lezione 7 - 20/10/2022\par
Slide 6\par
16 -> R_i is the rank of instance i (after removing instance 1 because it is useless). Rank is the index of the difference after sording them from the one with the lowes abs diff. to the one with the highest abs diff.. You can also average the ranks of instences of the same value.\par
\par
Slide 7\par
1 -> Constructive algorithms. Sometimes they are exact, but when? We'll see. And if they are not exact? We'll see how to modify them to try to make them exact.\par
2 -> A C.O. problem is a problem that has as solutions subsets of the ground set. We can start fron an empty subset as initial solution. If we have a proof that some elements of the G.S. must be in the best solution possible, we put them there immediately.\line This algorithm stops when a termination condition holds.\line Some elements of the GS are acceptable, other are not. i.e: knapsack. If the resitual capacity is not enough, I cannot add it to my partial solution. So I have a subset of acceptable elements, so I select some of them. \par
3 -> The construction graph is given by certain nodes and arcs (it's a directed graph). Nodes are subsets of 2^B (collection of all subsets). The arc set is the collection of all paris feasible solution, feasible solution united with i... The idea is: a node is something i can have during the execution of the algorithm, an arc is a move I can do to change my solution. It's a graph where I can't go back to an old solution once I abandoned it. \par
4 -> when do we stop the algorithm? When the current node (subset) has no arcs going out. It's as saying: whatever I add to the subset, brings me to a infeasible subset. Often, only the last subset is the only feasible solution. There are also cases in which we can get in and out of the feasible solutions subset by adding elements. \par
\par
Lezione 8 - 21/10/2022\par
21 -> Characterization in the additive case. Assuming additive objective function and the solutions being bases (maximal subsets, can also be considered "interesting solutions"), we can conclude some things. A constructive/greedy algorithm finds always a optimal solution iif B,F is a matroid embedding. For us, it's just useful to understand that the problem is solved under these assumptions. \line Most heuristics try to satisfy these properties (greedoids and matroids or ...)\par
22 -> Given pair (Ground set, Search space), we have a greedoid if three axioms are respected:\line -trivial axiom: empty set belongs to seatch space.\line -accessibility axiom: if a subset is acceptable and not empty, then there is an element that we can remove from the set and still find an acceptable elements. Basically it tells you that you can get in any element of the search space with this constructive algorithm.\line -exchange axiom: you have two asceptable subsets and one of them has cardinality of the other + 1. Then at least one of the elements of x - y should be added to y and get an admissible solution. (for example, this doesn't work for the knapsack 0-1, but it does for the fractional knapsack). A corollary to this: all bases have the same cardinality.\line Prim respects this\par
23 -> Small complication (now we have a matroid). Keep the first and third axiom, and change (but strenghten) the second one. Instead of accessibility, we want hereditarity. If we have a subset, then every subset of it is an acceptable subset. The knapsack problem, for example, respects this. \line Kruskal respects this.\par
25 -> Kruskal is a matroid. But why Prim works then? Because has a strong exchange axiom. Satisfies the trivial and accessibility axiom. The problem of Prim is hereditariety. It is compensated by strong exchange axiom. (Strong exchange axiom: not asked in exam).\par
Slide 8\par
2 -> when we can't find a good search space, we can change the selection criterium. Why don't we have a good search space in the kp problem? because of capacity constraints. So, we have solutions made of few objects and others made of many items. \line The greedy version is sometimes good, sometimes very bad, but by just changing the selection criterium we can have a 2-approximated algorithm.\par
3 -> possible exam exercise. To solve the problem, instead of maximizing the value, let's compute the solution with a given criterium. \par
6 -> TSP: first axiom and accessibility works. For exchanges? no, see slides of previous lesson (slide 7). \par
7 -> We start an empty path and try to find a hamiltonian cycle. We have the Nearest neighbour heuristic.\par
9 -> 48  states of US (we were in 1949).\par
14 -> a_i(x). how many NEW rows do i cover? (because that's the important thing)\par
16 -> It can be proved that  the algorithm is log_n(numer of rows) approximated.\par
17 -> This proof won't be asked.\par
18 -> Same thing. The algorithm is natural.\par
20 -> The objective function that measures the bins used is extremely flat. \par
23 - Consider the volumes from the one with highest volume to the one with lowe volume.\par
\par
Lezione 9 - 27/10/2022\par
Slide 9\par
2 -> Extensions to constructive algorithms. Instead of adding just one element, we add more (in one step). Or, we can add but also remove elements from the current solution. But this allows to come back to old solutions, which is inadmissible in constructive heuristics. \par
4 -> Is it a CO problem? (of course it is, but try to characterize it!)\line We MUST span all red vertexes, but we can include in the solution also black vertexes. \line Ground sets? For example, edges. Search space? Well, we can explore the search space following the ideas of Kruskal and Prim. But this gives problems.\par
5 -> So we can use the Distance heuristic. We basically use the Shortest Path for finding ways to connect the special vertexes.\par
9 -> epsilon is just a really small number, very close to 0. \par
10 -> Now, let's see second way of extending constructive heuristics.\par
19 -> Now, Nearest Insertion heuristic. We first choose the node, than the arc. \par
25 -> Now, Furthest Insertion heuristic. Similar to choosing the biggest item in bin packing. \par
33 -> Concept of regret: the idea is that we take a decision now, and it can affect badly future decisions. CMST: capacitated minimum spanning tree problem. The root is given. \par
35 -> Take a vertex and put it in the subtree (what we are doing).\par
36 -> Roll-out heuristics. \par
\par
Lezione 10 - 28/10/2022\par
The roll-out heuristic is, for us, not considered a meta-heuristic, since it doesn't have randomization nor memory. \par
39 -> Destructive heuristics. Instead of starting from empty set, we start with the overall set. Starting from x, remove a certain element i from it, depending on what is the best one to remove. We don't remove an element when it gets us outside from the search space.\par
40 -> Tipically a destructive herustic takes longer than a constructive heuristic. Also, the test for adding is easier than the test for removing (feasibility test).\par
41 -> For the SCP, some heuristics give a redundant solution. If constructive heuristics give a redundant solution, it could be better to use a constructive-destructive heuristic. Build a solution, but then apply a destructive heuristic from that solution. This is different from a normal destructive herustic because we don't have all the elements of the Ground Set here, but just those of the solution found. \par
\par
Lezione 11 - 03/11/2022\par
2 -> Most of the times, constructive heuristics have no guarantees. So the idea now becomes: if I have something that works, and change it a bit from run to run, why can't I execute it multiple times? "l" will be our index of iteration. Each iteration uses a different algorithm and selection criterium. We'll see three meta-heuristics. Adaptive Research Technique (ART), Greedy Randomized Adaptise Search Procedure (GRASP) and Ant Systems.\par
3 -> In principle, metaheuristics could go on forever. So, we have to introduce, from the outside, a termination condition. \par
\par
Lezione 12 - 04/11/2022\par
There is not just one path from empty set to optial solution, but there might be more than one. If we use randomization, we also have chance to (probabilistically) reach the optimum.\par
26 -> j at the denominator, not i.\par
27 -> the same.cd kn\tab\par
\par
Lezione 13 - 10/11/2022\par
2 -> Exchange algorithms. We throw away and add some elements. This is a general scheme. We start from a subset (usually a feasible solution). Is it always a feasible solution? Well, usually, yes. How do we obtain a initial feasible solution? It can be trivial or hard. To find it, we can generally use a construictive heuristic to build the solution. Then, we try to consider a family of possible considerations. If we have a set A (add) and a set D (delete) for elements to add and elements to remove, we have some moves we can do. For each solution x, we have a family of neighbour solutions. \par
3 -> neighborhood: a function that associate to each feasible solution a subset of feasible solutions N(x), that are still feasible. This time, basically, the graph (not construction graph) is composed of ndoes which are only feasible solutions, and the edges connect solutions with neighbour solutions. This graph, the SEARCH GRAPH, has cycles. \par
4 -> We need a way to produce neighbour solutions efficiently and sistematically. We'll see two methods to do so. The first one is based on the distance between solutions. The distance is sometimes also called hamming distance, and depends on the incidence vectors that represent the solutions. At this point, the neighbours of a solution are all those solutions (feasible) such that the distance between the two is not greather than a certain value k. This is a parametric definition of k. "How large is the neighbourhood?" could be a question (feasibility and hamming distance is the important stuff). \par
6 -> Another definition of neighbourhood: operations. How many operations do I have to eprform on the incidence vector to go from a solution to another one?\line N_H1 is "the set of neighbours with hamming distance of 1".\par
7 -> Assume here that the solutions are build from a B that is the set of arcs. \par
8 -> quite often an operation based neighbourhood gives an idea about what we are doing. But, sometimes, it corresponds to a hamming distance. \par
9 -> Different groud sets give different neighbours. Changing ground set can change drastically the distance between solutions. \par
10 -> Transfers are not always swaps. \par
11 -> What makes a neighbourhood good or bad? Here are some properties:\line 1) the neighbourhood should allow to reach the global optimum. We can have a graph weakly connected to the optimum or strongly connected to the optimum. \par
17 -> SQD: what is the probability of being at a certain distance from the global optimum?\par
\par
Lezione 14 - 11/11/2022\par
22 -> the denominator is the sample variance. It describes the variations, wince we compute, for each step, how far we are from the average, we square so to enhance big values and have only positive things, and average with the number of elements we have collected. But this depends on the unit of measure. The numerator, on the other hand, it's another thing. It's basically ana verage of the product in the part that is most above. \par
24 -> The presence of plateaus can be terrible for the steepest descent. \par
25 -> Attraction basin. You start from a solution and move deterministically (we don't have randomness right now) until the local optimum is reached. \par
\line slides 14\par
2 -> The complexity is given by how many iterations are necessary before finding a local optimum. We could find out, a posteriori, that the complexity is somewhat related to the dimension of the problem, or the number of elements of the search space. Optimizing in a small neighbourhood takes usually less time than exploring in all the search space. This takes, usually, a polinomial time. So, we have two strategies to explore the negihbourhood:\par
3 -> evaluate all the neighbour solutions. The complexity of this sort of exhaustive search is dependant on the number of neighbour solutions. \par
4 -> I take a solution. Given the extended neighbourhood i can generate with a flip, hamming distance, swap, or whatever, check the value of the objective function. Then I take the optimum. The complexity depends on the number of subsets visited, the time to evaluate their feasibility and the time to evaluate the objective for a feasible solution. We assume that the time required to generate the neighbours is constant, so not important for us. \par
5 -> To compute in a fast way f(x') for the neighbours, we can, instead of computing it from scratch, try to start from the f(x) of the initial solution given, and then add7subtract something, or anyway perfomr  a constant time operation. One thing to note is that, if the function is additive, the cost depends only on the object to remove and the object to add. \par
7 -> They commented the red trick and their algorithm required one day. Decommenting it, it took 1 second. \par
10 -> What happens if we sxchange the tasks connected by the... "can't remember what i was thinking when i made this picture". \par
13 -> the last two parts of the red formula are a sequence of costs. \par
15 -> Branch: subtree appended to the root. Subtree: a general subtree (this is a terminology note). \par
\par
Lezione 15 - 17/11/2022\par
16 (the number is the one on the right bottom) -> sometimes we don't need to exhaustively explore all the neighbours.\par
17 -> All part of the solution that don't change iteration after iteration can be stored and stored somewhere. \par
18 -> when solutions are partitions (BPP, CMSP, Vehicle Routing Problem), we have that some exchanges from different solutions give the same results. This is a way to explore quickly but exhaustively the neighbourhood.\par
15\'b0 slide\par
2 -> A larger neighbourhood gives a bigger attraction basin, and the steepest descent becomes effective even though the exploration time is longer. Let us consider a larger neighbourhood (exponential or a high polynomial, not more than 4th power usually), but explore it in a pow-order polynomial time. To do so, we can select a neighbourhood that we can explore efficiently with an exhaustive algorithm, or explore it algorithmically. \par
3 -> we would like a large k when we need to find a better solution because it is difficult to find it. Otherwise, we are ok with a small k. \par
5 -> Performing the red swap and then the blue swap or the opposite is the same. Those specific swaps are compatible (the order doesn't matter). \par
6 -> The idea is that we take mutually independent possible moves and compose them. \line Rows: parts of the solution. Colums: elementary moves. We want a set of columns with the maximum effect (variation of objective function, and must be negative since the problem is of minimization) and being a packing (we don't want two moves to affect the same part of the solution). Problem? The Set Packing Probelm iw NP-hard usually. \par
7 -> Once again, we have an auxiliary problem and a set of moves. But this time, we have a sort of sequence of moves. Suppose that the solution is made of objects divided in components. \line Problem with transfers: sometimes they are infeasible. In the BPP, sometimes we cannot move anything froma  container to another one. And this is bad for an exchange heuristic. In those cases, we say: we don't do just one transfer, but two (a swap). But why can't I do a swap with three elements?\par
8 -> Here, nodes are single elements of a solution. Arcs correspond to transferring an element from its component to another component. (i,j) means i take element i and put it to the component where there is k. Also, I take away j. But we can't do that. To still be feasible, we have to say that j goes somewhere else, and so on. This will become feasible when we go back to the original node, i. \line It's not always to move i to the component of j, so we can only consider the arcs that produce feasible solutions. The cost of an arc is the difference between the new cost of the component and its old value. \par
10 -> So, we are searching for a minimum cost circuit. And this problem is NP-hard. If we have the constaint that we can visit each component only once, then we can have a efficient dynamic programming algorithm that grows partial paths. The final result must be negative. if at a certain point in time we reach a node and we have a positive cost, there is a mathematical property that says that the positive terms can be thrown away. not l but k on the sum over... at the end of the slide.\par
12 -> Cyclic exchanges mantain the same cardinality in all the components, so we are bound in a part of the feasible region X. Can we do something else? Yes, we can do noncyclic exchange chains. \par
\par
Lezione 16 - 17/11/2022\par
So, we were talking about move combinations. \par
17 -> Variable Depth Search. We make a variety of moves also this time. If we don't fix the length of the solution, we can't explore all the neighbours efficiently (generally speaking).\par
Does all of this make sense? If the results are extremely better (in the TSP for example) and the complexity is still good, yes. \par
\par
Simplest neighbourhood for the MDP? Swaps. They have hamming distance equal to 2. \par
\par
\par
Lezione 17 - 24/11/2022\par
3 -> To get to the local optimum, we can have different execution times in different runs. So it's important to fix the termination condition to a maximum amount of time or maximum number of repetitions of the basic scheme. \par
4 -> Sometimes, the new starting solution is just a modification of the best solution found by the last execution of the algorithm. \par
9 -> the two following methods are actually the same method, according to the Professor.\line Start with the usual steepest descent You get a local optimum and save it. This is the basic exchange heuristic. From this local optimum, we apply a perturbation. We apply the SteepestDescent to it to get a new solution. If it is better than x*, we update x* with this, and repeat from there. Else, we repeat from the last local optimum.\par
11 -> Problems: what if I go back always to the same solution? \par
18 -> Here the neighbourhood varies. \par
\par
\par
\par
\par
\par
\par
La scatola a destra ti interessa solo quando il suo x0 \'e8 <= di x0 + xlen della scatola dietro.\line In questo caso, ti interessa (della scatola a destra) sia il suo z che il suo y.\par
La scatola davanti ti interessa solo quando il suo z0 \'e8 <= di z0 + zlen della scatola a sinistra.\line In questo caso, ti interessa (della scatola davanti) sia il suo x che il suo y.\par
La scatola sopra ti interessa quando il suo z0 + zlen <= z0 + zlen della scatola a sinistra //e di quella a destra (se quella a destra ti interessa)\line + La scatola sopra ti interessa quando il suo x0 + xlen <= x0 + xlen della scatola dietro\line In questo caso, della scatola sopra, ti interessa sia il suo x che il suo z.\par
\par
\par
\par
\par
\par
\par
}
 