{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1040{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.22000}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\qc\f0\fs22\lang16 Appunti di Heuristic Algorithms\par

\pard\sa200\sl276\slmult1\qj Lezione 1 - 29/09/2022\par
[he wanted us to download the lessons]\par
Heuristics algorithms are not for specific problems. They are general ideas that work widely for any possible problem. Then those ideas are adapted to the particual problem. We'll see how we adapt general ideas to specific problems and then evaluate if the algorithm is good or not. First, some historical ideas. Then, we'll see that "heuristics" are used in many contexts. Then, how to evaluate the performance. Heuristic comes from greek that means "I find", based on the famous story of Archimetes and the golden crown. He wanted to understand if that crown was authentic or not. it was based on the volume, measured putting the crown in a "pool". The concept of heuristics go back to the 4th century a.c. about the ideas that help find proofs for math theorems. These ideas were used in 17th century onward. Heuristics were a collection of general ideas, intuitions.\par
The idea of the heuristic algorithms is that we cannot ensure that we will have a good solution, but most of the time will be a good solution (or the correct/best solution). We can use distributions that tell us how much it is likely that we'll get a good solution. We'll talk about some heuristics for some problems. We consider only Combinatorial optimization. We'll only consider heuristics based on solutions and not models. \line Problems in fact can be classified based on the nature of their solution:\line -Decision problems: true or false.\line -Search problems: the solution is any feasible subsystem (that is, satysfing certain conditions).\line -Optimization problems: The solution is the minimum or maximum value of an objective function defined on the feasible subsystems. Usually, optimization and search are tied together.\line -Counting problems: number of feasible subsystem.\line -Enumeration problems: collection of all feasible subsystems.\par
The solutions in a CO problem is a combination. So, a CO problem can also be considered as a Subset Optimization problem. A combination is: we have n elements and take k of them. \par
Solution based heuristics: heuristics where we use and manipulate subsets of the ground set. \par
Metaheuristic is the common name for heuristic algorithms with randomization and/or memory. \par
5 parts of the course. This lesson and tomorrow one is the first part. We concern on what is a CO and its ground set. Also, is your algorithm good or not? This can be discussed at priori or after executions. We can also give a lower bound on the performances of an algorithm. "My algorithm won't give a solution worse thatn half of the optimum", for example. \par
Let's start with a suervey of problems that are examples. \line 1st: knapsack problem. The ground set is the set of objects to put in the knapsack, because a subset of them is a (possible) solution. From now on, we will distinguish the solutions between Feasible and Unfeasible. \line Problem of three points most distant: a ground set (compact) is the set of all points. Another one (that uses the trick) is the set of all triples. How do we represent the FEASIBLE solution? \{A,B,C\} as a vector is an example. Antoher one is an incidence vector: [1,1,1,0,0,0,0] (1 for the points i count, 0 otherwise). \par
In the knapsack, the objective function is ADDITIVE. This means that in the function we have a sum of all the elements of the solution with a function (?).\par
Flat objective functions work bad with the BPP and the PMSP when the exchange heuristic is used, but the same heuristic is good for the Knapsack.\par
\par
Lezione 2 - 30/09/2022\par
Yesterday, we saw four problems of combinatorial optimization. Some questions were: what is the ground set? There are multiple answers and some ground sets are better than others. Let's also ask ourselves some questions about the combinatorial optimization problem. They are related on the representation of the solutions and such. \line Flat representation: bad for exchange heuristics, since different solutions may have the same value.\par
Max-SAT problem. A logical variable is a variable that can be true or false, can only assume two values. Logical variables can be used in logical functions. \par
...\par
16 -> ground set for SAT: for example, cartesian product of variables and B (0,1). Another one, simpler, is one that takes as subset only the true variables. The objective function is not additive since is not a sum over the elements of the solution. We are summing something that is not related to (x_i, b), but to the conjunction of OR formulas. \par
18 -> Do all the tasks (rows) with minimal cost (value for each column). Ground set: set of columns. A subset of them is a possible solution. The OF is additive.\par
19 -> The feasibility test. Is this a subset or a feasible solution? important question. \line Exchange can create feasible or unfeasible solutions, really depends. in PMSM exchange makes always feasible solutions, in BPP it might not. \par
Set covering: each row must be covered at least once.\par
Set packing: each row must be covered at most once.\par
Set partitioning: each row must be covered EXACTLY once. (we always want to maximise the value of the sum of columns).\line\b Can you find a feasible solution? Is there an easy way to find it?\b0  Exchange heuristics start from a solution. Recombination heuristics start from a population of solutions. In this case, it is hard. \line Set covering -> take all columns (first solution)\line Set packing -> take no column (first solution)\line Set partitioning -> Difficult. Feasibility is NP-complete. One thing we can do is: I'm not sure I can find an element in X, but thanks to a relaxation of the feasible region (X') maybe I can find a feasible solution for the relaxation. \par
So, finding a feasible solution and understanding if a solution is feasible is a main concern. \par
25 -> Graph problems: Vertex Cover Problem. Find a set of vertexes that cover all the edges. An edge is covered if one of the vertex it touches is selected for the solution.\par
35 -> can we find a feasible solution? It depends. Is there a Hamiltonian circuit? This is a strong NP-hard question. But if a complete graph is given, we can take any permutation and it is a possible solution. \par
\par
Lezione 3 - 06/10/2022\par
37 -> This problem (CMSTP) is used for connecting servers. NP-hard to find optimal solution. In linear time (with respect to the set x), we can see if the solution is feasible or not. The obj function is the sum of functions over elements of the solution, so it's additive. Also for exchange heuristics is good, but if we exchange an edge with another we have to re-visit most of the graph to see if it is feasible. \line Here we have vertexes that we want to put in subtrees (BPP ispossib\'f2l?). \par
39 -> (Vertex, Subtree). Also with this ground set we can represent a solution. Checking the feasibility? We just count how many vertexes are in a subtree. But this is ok for not exceeding W. We also have to check if all the nodes of the subtree are conneceted and at least one is connected to r. This is more complex. BUT, if it is a complete graph, it's much easier. So, IN SPECIFIC SUBSET INSTANCES OF THIS PROBLEM, this representation is good (very good). How do we compute the cost anyway? We should have edges, that are not given in the representation. So, in order to compute the objective value of a solution x, we have to resolve the MST problem for each subtree. I take the cheapest tree that contains all the nodes belonging to that representation. (MST = Minimum Spanning Tree). Here, the complexity of computing the objective function is not linear, but is costly (not a lot anyway). \par
44 -> So, which representation is the best? Depends. What am I going to evaluate more times? Feasibility or obj. function? That should be the simpler. One can use both representations! ...but that doubles memory and updates to a solution. Is this a disadvantage? Depends. Sometimes, it is convenient to have two representations. It's true that they must be consistend, but one allows easy computation for feasibility and the other for the objective function. \par
43 -> x it's a feasible solution if each vertex has an ongoing and outgoing edge in x. Also, check if the cycles created don't exceed the maximum capacity. Using the other representation: it's easy to compute the weight of each sub-cycle, and if it is complete as a graph it's easy to find the cycle. But, obj function? Solve the TSP for each sub-cycle (NP-complete :( )\line But when we talked about the Max-SAT, we proposed x = subset of variables made true. Another proposal was x = clausles made true. In this last case, the feasibility is SAT. I mean, you'd have to compute a SAT solution to understand if x is a feasible solution for a SAT problem. That's stupid. \line But in 43, if the graph is complete and W is small, it is true that we need to solve TSP for sub-problems, but the instances are so small that we can do it! But the small TSP, how is computed? Exactly or with another heuristic? eh...\line It has been shown that using heuristic to solve those small TSP can lead to good solutions. \par
Slide 03:\par
A priori and a posteriori analysis. Today we talk about cost, tomorrow about quality. \par
2 -> A problem is made by many cases, that are all similar. I have infinite instances, possibly. For each instance, i have a solution. The problem can be seen as a function! I -> S. The problem is a function that maybe we can compute (depends on the instance), but maybe no. \par
3 -> We can have different algorithms for the same problem. \par
4 -> Cost is time and space. It is possible to reduce the use of time by allowing the use of more space and vice-versa. \par
13 -> What if the capacity of KP is small? Well, in that case, even if I have a lot of items (n), the problem is easy, because in general at most two items can stay in the knapsack. In this case, the complexity is quadratic. \par
circa 16 - Parametrized complexity. We have a parameter that affects the complexity.\par
\par
Lezione 4 - 07/10/2022\par
13 and around -> find the worst instance not only with respect to n, but also with respect to k, another parameter. \par
24 -> after describing the probabilistic model, you have to find the expected value over each of the slices, for each value of n. You can build a probabilistic model based on simulations. \par
25 -> Let's talk about (binary) matrices. For the Set Covering Problem, we had columns that cover rows. How can we give a probabilistic model for those matrices?\line -equiprobability: there are 2^mn matrices. If I assume that each of them has the same probability, i compute the complexity starting from this assumption.\line -uniform probability: each cell is set to 1 with a certain probability\line -fixed density: extract a certain numner of cells with a uniform probability and set them to 1. Dense matrix = fast algorithm, sparse matrix = slow algorithm. In fixed density, we have exactly delta ones. In the equiprobability, the process is still random.\par
26 -> the first case is a particular case of the second. \par
29 -> Sat exhibits phase transitions. OCCHIO: the blu part on the right has the same role of the red one on the left, and viceversa. \par
31 -> In heuristics, the concept of complexity is a bit different than the usual one.\par
lez_04\par
2 -> If efficency means low cost, effectiveness means quality of the solution. So, we need the concepts of closeness and frequency. We can also do a priori and a posteriori analysis. A priori means: I prove that my algorithm also gives a solution that has a certain quality.\par
3 -> There are three definitions for the effectiveness of a heuristic optimization algorithm. f_A(I): value given on instance I by my heuristic algorithm A. I'd like this value to be equal to f*(I), the true optimal value of the instance. \line What if you don't know the optimum? You can construct a lower and upper bound for the optimum. \line (Lower bound = dual bound, upper bound = primal bound). \par
4 -> Up until now, the reasonings were based on experimental analysis (particualr instances). Let's do a thoeretical analysis. \line For the minimum coloring graph problem, the minimum number of colors you have is either the dimension of (the maximum degree) the max clique in the graph or that value +1. \line The last inequality tells us that what our heuristic finds is always bounded by a function of n.\par
\par
Lezione 5 - 13/10/2022\par
We've seen a priori study of heuristic algorithms. In particular, a priori cost and effectiveness. \par
4\'b0 slide\par
14 -> Better approx algos but worse time, there is this trade-off in some algorithms. Not all problems can actualy be approximated. \par
13 -> example: the TSP. Inapproximable = can't prove that your algorithm is alpha times the optimum. \par
5\'b0 slide\par
2 -> empirical evaluation of a heuristic algorithm. The thoeretical analysis is complicated, and a precise answer might not be practical. So, we can follow an experimental approach.\par
3 -> Experimental approach is: bserve the field of your study, then build the model. Then, design experiments to evaluate the model, analyse the results and revise the model based on the results until a satisfactory model. The model could be debunked, in that case you modify it. Or, your experiments prove your system to be correct. \par
4 -> we want to find indicies of efficiency and effectiveness of an algorithm. Then, use this indices to compare different algorithms. In this way, you conlcude that an algo is better than another one. Maybe my algo is good on small instances but good on greater instances.\par
5 -> how do we perform experiments? giving a benchmark of instances. Ideally, it is infinite. practically, it's a subset. What subset? It must be representative of the whole set, relating to structural features. The instances should not be too easy or too hard\par
6 -> the environment influence the result and makes it difficult to reproduce precisely the result. \par
7 -> we sould describe the cost and quality of our algorithms. The only time in which we can say that we prefer an algorithm over another is when it takes not much time and the quality is good. Otherwise, the time or the result must be similar to compare the other feature. \par
8 -> statistical model of algorithm performance? We have the domane of instances, and from it we extract a sample set/benchmark. From it, we randomly extract an instance each time we need one. T_A(I) is the computational time that is a random variable. The relative difference delta is also a random variable. \par
9 -> When i try to compute the relative difference, i usually don't have the optimal value So, we have to estimate delta_A(i). So, we need LOWER BOUND and UPPER BOUND, eccoliiiiiiiiii.\line The lower bound is not greater thatn the optimal, while the upper bound is greater than the optimal. \par
10 -> We'll see a lot of pictures. Le'ts see the Runtime Distribution Diagram. It's a plot that on x axis gives a time, on y gives how likely it is to solve an instance that takes that time. Bascially: given instance i, and fixed time x, y is the probability that i will be solved in that time. \par
11 -> Main features: monotone nondecreasing. It's also not continuous. In zero or before 0 the diagram is equal to zero (you need some time to solve the problem). It's also equal to 1 from a certain point on. \par
13 -> up till now, we assumed that the size was fixed. What happened if we allow time to change? Scaling diagram. x = size of the instance. y = time to solve it. \par
15 -> The Solution Quality Distribution is a diagram of the relative difference. \par
\par
Lezione 6 - 14/10/2022\par
Slide 5\'b0\par
10 -> it's a probability distribution function. \par
14 -> number of elementary operation you perform but also time (?). Depends on the computer you do your runs. \par
15 -> Should we fix the size of the instances or not? Depends. A thing you could do is: consider different sizes, and get profiles of the relative differences. \par
18 -> parametric diagrams. Let's fix the size of the instances and build, for each of them, the SQD diagram. This might show us a trend. As you increase the size of your problem, your algorithm becomes less precise and more heuristic (this is something that can happen). \par
19 -> you can use the SQD to compare the performances of different algorithms. Strict dominance: an algorithm is better than another when the relative difference of the first is better than the relative difference of the second on all instances. Strict dominance implies probabilistic dominance. \par
slide 6\'b0\par
2 -> Compact statistical descriptions. Mean and Variance depend a lot on outliers, unfortunately. The average is used, the variance is less used.\par
3 -> The median divides in two the benchmark. Compute minimum, mazimum, median, etc.. might be a question. Usefulness of this diagram? First: we can observe that this diagram is a simplification of the SQD. Till the minimum, the SQD would be 0. from minimum to lower quartile, the SQD goes up to 25%. Then, from it to the median, goes up to 50%, and so on. From the SQD, you can have the box plot, too. We can use box plot to compare algorithms.\par
4 -> for different algorithms, we don't draw the SQD but the boxplot. The small circles are outliers: they are outside the box. Often happens that an algorithm gets a certain result, but sometimes gets out of the way. Ok mi torna che figoooo. Compare A7 and A8. What is better? A7. Why is it clearly better? because its box is ompletely below the other. This is strict dominance. For probabilistic dominance, we have A2 and A3. It's possible that we don't have strict dominance, but also the contrary is true. Wait, how do we impose probabilistic dominance? That each quarter is better than the previous one. No. Each quartile MUST preceed the corresponing of the other one, ok. But ALSO, it has to preveed the NEXT quartile of the following. \line ok aspetta, ho capito. il secondo del primo deve essere del tutto sotto il primo del secondo.\par
Il terzo del rpimo deve essere meglio del secondo del secondo.\par
il quarto del primo deve essere meglio del terzo del secondo, ok, quando questo \'e8 vero, hai probabilistic dominance. Il rpimo \'e8 probabilistic dominant sull'altro.\par
7 -> Using a randomized algorithm depends on instance, time and seed of the pseudo random number generator. \par
8 -> You can classify algos in at least three ways.\line 1) Complete/exact algorithms. For every instance, it gives the optimum in finite time. The time depends on the size of the instance. Every instance basically has a time after which we are sure that the algorithm gives us the optimum. \par
2) Probabilistically approximately complete. Running the instance for a long time gives us the optimum or not? The probability converges to 1 as the time goes to infinity. SOME meteheuristics don't fall in this cathegory. Is this property interesting? \par
3) Essentially incomplete: finds the optimum for some instances with probability strictly lower that 1 as t goes to infinity. For many other instances, finds the optimum with pro = 1 when t goes to infinity.\par
9 -> alpha complete algorithm is an approximation algorithm. bla bla bla\par
11 -> Fix a certain quality. What happens for a certain time?\par
prendi il grafo di destra, x = 10, y = 30, dotted line: Il 30% delle istanze risolte in maniera ottima vengono risolte in meno di 10 secondi. La linea continua a crescere perch\'e9, se il 30% delle istanze le risolvi in tempo x, le risolvi di sicuro anche in tempo x+c.\par
13 -> Median = 0.5\par
14 -> The idea is that we can compare two algorithms, but what if there is no probabilistic dominance?\par
4 again -> Very small box and large whiskers and viceversa? First, the box is half of the insances (average half). Small box = stable algorithm. \par
14 -> minimization problem. if the difference is always negative, strict dominance. if sometimes it is, sometimes it is not, no guarantee. \par
\par
Lezione 7 - 20/10/2022\par
Slide 6\par
16 -> R_i is the rank of instance i (after removing instance 1 because it is useless). Rank is the index of the difference after sording them from the one with the lowes abs diff. to the one with the highest abs diff.. You can also average the ranks of instences of the same value.\par
\par
Slide 7\par
1 -> Constructive algorithms. Sometimes they are exact, but when? We'll see. And if they are not exact? We'll see how to modify them to try to make them exact.\par
2 -> A C.O. problem is a problem that has as solutions subsets of the ground set. We can start fron an empty subset as initial solution. If we have a proof that some elements of the G.S. must be in the best solution possible, we put them there immediately.\line This algorithm stops when a termination condition holds.\line Some elements of the GS are acceptable, other are not. i.e: knapsack. If the resitual capacity is not enough, I cannot add it to my partial solution. So I have a subset of acceptable elements, so I select some of them. \par
3 -> The construction graph is given by certain nodes and arcs (it's a directed graph). Nodes are subsets of 2^B (collection of all subsets). The arc set is the collection of all paris feasible solution, feasible solution united with i... The idea is: a node is something i can have during the execution of the algorithm, an arc is a move I can do to change my solution. It's a graph where I can't go back to an old solution once I abandoned it. \par
4 -> when do we stop the algorithm? When the current node (subset) has no arcs going out. It's as saying: whatever I add to the subset, brings me to a infeasible subset. Often, only the last subset is the only feasible solution. There are also cases in which we can get in and out of the feasible solutions subset by adding elements. \par
\par
Lezione 8 - 21/10/2022\par
21 -> Characterization in the additive case. Assuming additive objective function and the solutions being bases (maximal subsets, can also be considered "interesting solutions"), we can conclude some things. A constructive/greedy algorithm finds always a optimal solution iif B,F is a matroid embedding. For us, it's just useful to understand that the problem is solved under these assumptions. \line Most heuristics try to satisfy these properties (greedoids and matroids or ...)\par
22 -> Given pair (Ground set, Search space), we have a greedoid if three axioms are respected:\line -trivial axiom: empty set belongs to seatch space.\line -accessibility axiom: if a subset is acceptable and not empty, then there is an element that we can remove from the set and still find an acceptable elements. Basically it tells you that you can get in any element of the search space with this constructive algorithm.\line -exchange axiom: you have two asceptable subsets and one of them has cardinality of the other + 1. Then at least one of the elements of x - y should be added to y and get an admissible solution. (for example, this doesn't work for the knapsack 0-1, but it does for the fractional knapsack). A corollary to this: all bases have the same cardinality.\line Prim respects this\par
23 -> Small complication (now we have a matroid). Keep the first and third axiom, and change (but strenghten) the second one. Instead of accessibility, we want hereditarity. If we have a subset, then every subset of it is an acceptable subset. The knapsack problem, for example, respects this. \line Kruskal respects this.\par
25 -> Kruskal is a matroid. But why Prim works then? Because has a strong exchange axiom. Satisfies the trivial and accessibility axiom. The problem of Prim is hereditariety. It is compensated by strong exchange axiom. (Strong exchange axiom: not asked in exam).\par
Slide 8\par
2 -> when we can't find a good search space, we can change the selection criterium. Why don't we have a good search space in the kp problem? because of capacity constraints. So, we have solutions made of few objects and others made of many items. \line The greedy version is sometimes good, sometimes very bad, but by just changing the selection criterium we can have a 2-approximated algorithm.\par
3 -> possible exam exercise. To solve the problem, instead of maximizing the value, let's compute the solution with a given criterium. \par
6 -> TSP: first axiom and accessibility works. For exchanges? no, see slides of previous lesson (slide 7). \par
7 -> We start an empty path and try to find a hamiltonian cycle. We have the Nearest neighbour heuristic.\par
9 -> 48  states of US (we were in 1949).\par
14 -> a_i(x). how many NEW rows do i cover? (because that's the important thing)\par
16 -> It can be proved that  the algorithm is log_n(numer of rows) approximated.\par
17 -> This proof won't be asked.\par
18 -> Same thing. The algorithm is natural.\par
20 -> The objective function that measures the bins used is extremely flat. \par
23 - Consider the volumes from the one with highest volume to the one with lowe volume.\par
\par
Lezione 9 - 27/10/2022\par
Slide 9\par
2 -> Extensions to constructive algorithms. Instead of adding just one element, we add more (in one step). Or, we can add but also remove elements from the current solution. But this allows to come back to old solutions, which is inadmissible in constructive heuristics. \par
4 -> Is it a CO problem? (of course it is, but try to characterize it!)\line We MUST span all red vertexes, but we can include in the solution also black vertexes. \line Ground sets? For example, edges. Search space? Well, we can explore the search space following the ideas of Kruskal and Prim. But this gives problems.\par
5 -> So we can use the Distance heuristic. We basically use the Shortest Path for finding ways to connect the special vertexes.\par
9 -> epsilon is just a really small number, very close to 0. \par
10 -> Now, let's see second way of extending constructive heuristics.\par
19 -> Now, Nearest Insertion heuristic. We first choose the node, than the arc. \par
25 -> Now, Furthest Insertion heuristic. Similar to choosing the biggest item in bin packing. \par
33 -> Concept of regret: the idea is that we take a decision now, and it can affect badly future decisions. CMST: capacitated minimum spanning tree problem. The root is given. \par
35 -> Take a vertex and put it in the subtree (what we are doing).\par
36 -> Roll-out heuristics. \par
\par
Lezione 10 - 28/10/2022\par
The roll-out heuristic is, for us, not considered a meta-heuristic, since it doesn't have randomization nor memory. \par
39 -> Destructive heuristics. Instead of starting from empty set, we start with the overall set. Starting from x, remove a certain element i from it, depending on what is the best one to remove. We don't remove an element when it gets us outside from the search space.\par
40 -> Tipically a destructive herustic takes longer than a constructive heuristic. Also, the test for adding is easier than the test for removing (feasibility test).\par
41 -> For the SCP, some heuristics give a redundant solution. If constructive heuristics give a redundant solution, it could be better to use a constructive-destructive heuristic. Build a solution, but then apply a destructive heuristic from that solution. This is different from a normal destructive herustic because we don't have all the elements of the Ground Set here, but just those of the solution found. \par
\par
Lezione 11 - 03/11/2022\par
2 -> Most of the times, constructive heuristics have no guarantees. So the idea now becomes: if I have something that works, and change it a bit from run to run, why can't I execute it multiple times? "l" will be our index of iteration. Each iteration uses a different algorithm and selection criterium. We'll see three meta-heuristics. Adaptive Research Technique (ART), Greedy Randomized Adaptise Search Procedure (GRASP) and Ant Systems.\par
3 -> In principle, metaheuristics could go on forever. So, we have to introduce, from the outside, a termination condition. \par
\par
Lezione 12 - 04/11/2022\par
There is not just one path from empty set to optial solution, but there might be more than one. If we use randomization, we also have chance to (probabilistically) reach the optimum.\par
26 -> j at the denominator, not i.\par
27 -> the same.cd kn\tab\par
\par
Lezione 13 -> 07/11/2022\par
Lagrangian relaxation. Before, we saw linear relaxation and its consequences.\par
2 -> We'll use LR in a branch and bound algorithm. \par
3 -> negative -> the solution complies the contraint. Positive -> the constraint is violated (for the red b - Ax). The LR says: take the obj function. If a constraint is violated, its solution is penalized. Otherwise, it is rewarded. \par
5 -> You have an equality constraint? you can turn them into inequality constraints. \line Ax >= b => -Ax <= -b, and lambda = lambda' - lambda^2 (?)\par
9 -> pink line = slope of obj function.\line Imagine that the red constraint doesn't allow us to solve easily the problem. But if we remove it, it is easier. IMAGINE, ok? It's to show the geometric meaning of the relaxation. \par
10 -> max Z_lr is the lower (so upper in this case) bound, we want it as small as possible. So, we need lambda as small as possible. Extreme cases: 0 and +inf. With a value in the middle, we can change the slope of the obj function. It's like taking the convex combination of two objectives. In this PARTICULAR example, the optimal solution of the relaxation is the same as the one of the original problem, something that usually doesn't happen.\par
7 -> Lagrangean dual problem: what is the best value for lambda? (We assume that here the problem is started from a minimization problem, which is the opposite wrt the problem we saw earlier). Z* ld is the best possible dual bound (here maximum lower bound we can get from lagrangean relaxation. In the example before, the best lower bound was the lowes upper bound). \par
8 ->With LR, we can't say that the optimum of LR is also the optimal for P. To be optimal, we require both feasibility and complementarity. Complementary is implied by feasibility, if we had equality constraints (???).\par
11 -> fix lambda\par
13 -> fix x. We have an exponential number of lines. This is a linear programming problem with an exponential number of constraints. Can we use simplex? No, the input is too large. one constraint for each feasible solution of the og problem. From an algorithmic viewpoint, we need different methods.\par
14 -> 1/9 here is the best value for lambda. In x*, the value fo the upper bound is the value of the lagrangean relazation function computed in aaa (x* is a value between the two extreme points). In x*, the violation is 0. \par
Covexification: when we can solvea subproble of the original problem by convexifiyng its feasible region. \par
In general, the optimal value of the lagrangean dual is the max of the original function subkject to the relax constraint and the constraints defining the convex hull of the problem. With lagrangean relaxation, we are convexifying something yes and something not.\par
15 -> how good is LR?\line we have positive and negative results. The LR is between the best case and linear relaxation.\ul\par
\ulnone 17 -> The middle is "Lagrangead dual" (i think), while the one on the right is "Linear Problem".\par
6 -> He's talking about this slide (Observations)\par
\par
\par
\par
\par
\par
\par
\par
}
 