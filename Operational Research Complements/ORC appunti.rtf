{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1040{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.22000}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\qc\f0\fs22\lang16 Appunti di Operational Research Complements\par

\pard\sa200\sl276\slmult1\qj Lezione 1 - 29/09/2022\par
Recall of basic notions. \line The focus is on models and algorithms to solve complex probelms in mathematical programming. We want to solve NP-Hard problems. Operational Research mainly concentrated on models, and the only algo seen was the simplex algorithm for linear programming (Passacantando did also other things). here, we will also focus on how do design suitable algorithms for our problems. We'll FOCUS on algos for Mixed-Integer Linear Programming (some variables are discrete). Algos for Non-Linear Programming could be provided nontheless, though the Professor doesn't like them. \line This course is part of the Analytics and optimization stream: Combinatorial optimization, Heuristic algorithms for C.O. (efficiency not optimality) and Decision models and methods (more general, more objectives, two or more decision-makers, unvertainty in data). Also Simulation and Logistics are related.\line The exam consists of:\line -a project and an oral exam. The project requires to design, implement and test an algorithm to solve an NP-hard problem. 5 pages are enough for the report. The oral exam excludes the part of the project. Sometimes, a small written exam must be done before the development of the project, just to see if the student is going to waste its time or not.\line -another way to pass the exam is developing a paper about an algorithm and exposing it in a seminar. The seminar can basically replace the project.\line For CS people, the project might be better.\line One can also do a master degree thesis. The project can be the first chapter of the thesis.\par
Why attending and NOT attending this course? \line -We deal with optimization techniques. If we want to develop special skills for optimization algorithms. The optimization methods nowdays are at the base of many decision making systems (?) that are required nby many companies (consulting companies). "I'm a Operational Research Professional" doesn't make sense in Italy but it does abroad. \line -not attend this course if you are foreign and this is in english or you need credits with MAT label.\par
About the Operational Research Analyst: in the 2019...\line -n. of positions: 109700\line -Median annual salary: 84810 USD (+41% w.r.t. US median)\line -Forecasted growth 2018-2028: +26% (US: +5%)\line There is also the Certified Analytics professional that distinguish the TRUE people that knows the subject from those that just say they do. \par
Mathematical Programming.\line MP deals with optimization problems with no uncertainty, one objective and one decision-maker. We can further distingush this class in:\line -LP: linear constraints and objective, continuous variables.\line -MILP: as before but some variables are discrete.\line -Non-Linear Programming NLP: variables continuous, but constraints or objective are non linear.\line -MINLP: some variables are discrete and some constraints or the objective are not linear.\par
Let's recall some basics in Linear Programming.\line In LP we have variables with continuous domains, the constraints are linear equations and inequalities and the objective function is a linear function of the variables. (c^T x, not cx in the slides).\line We can always start from this formulation and replace equalities with inequalities (inequalities form). In this form, it's easy to give a geometric interpretation. In the geometrical interpretation of LP, equality defines an hyperplane and the inequality a halfspace. The constraints are conjuncted, and geometrically we have the intersection of halfspaces. This is a polyhedron. Polyhedron are convex sets.\line [[Following the slides]]\line If a problem is unbounded, the other is surely unfeasible, but the opposite is not true. If P is infeasible then also D might be unfeasible.\par
In discrete optimization probelms:\line primal bound = feasible but not optimal, dual bound = optimal but dot feasible\par
CPAIOR\par
Cplex, Gurobi, AMPL\par
\par
Lezione 2 - 03/10/2022\par
2a\par
5 -> you choose (a1, a2, ..., an)\par
9 -> For exploring in b&b, Depth-first (LIFO list) and Best-first (the list must be sorted) are better than Breadth-first (FIFO list). We also want to spend O(log(n)) for insertion and extraction. With linked list we would need O(1) for insertion and O(n) for extraction. To gain the first complesity, we can use a binary heap (tree). \par
10 -> branching produces big trees. Leaves are nodes that don't requrie further branching. \par
11 -> (Corollary 2) it he relaxation is the continuous one, it is true. (corollary 3) Here z^_ is just a random value, nothing to do with primal/dual bound. This corollary is the bounding of the b&b. \par
12 -> remember we are trying to minimize here, so the last line works. If the best solution of the relaxation is greater (worse) than the primal bound (feasible but worst) of the original problem, we close that node.\par
15 -> Producing leaves early is useful because it is a solution, and that is a feasible integer solution, that we can use as a primal bound. With DFS, we can heuristically find this feasible but sub-optimal solution basically. A recursive implementation of the B&B algorithm, the tree of activation records is managed by the OS. The algorithm becomes much faster in this way, rather than an explicit management of the Activation records. \par
2b\par
1-> We want the optimal permutation of items/tasks/objects/whatever.\par
4 -> the sum over all the j that follow i... so to compute those values we have to start from the last element of the permutation and backtrack to the start. \par
6 -> it's non-linear because of x_ij and alpha_j. \par
9 -> L = non fixed nodes (left). R = fixed nodes (right). \par
11 -> In the blu one, x_ij has already been fixed to 1.\par
12 -> LB_0 = Lower Bound 0\par
13 -> beta_i is a part of alpha_i. Can we estimate it?\par
17 -> at the root node, the gaps are computed (gaps = difference bw lower bound and optimum).\par
18 -> Largest at beginning, smallest at the end (it's an error).\par
19 -> p1 > p2 > p3 > p4\par
20 -> We want a measure of the effectiveness of the sorting policy. When do we find the optimal solution?\par
23 -> Size and time is similar, but the best solution is found before. It is useful if we want to truncate the search. Truncation is an important addition to branch and bound. We can set a timeout such that, when it is reached, the solver is stopped. The result is the best solution found so far. Usually one would like also to know the gap between the solution given and the lowe bound (dual bound). The smaller the gap, the better the solution (?). \par
26 -> Last line = number of optimal solutions found. \par
\par
Lezione 3 - 06/10/2022\par
Preprocessing -> things to do before b&b to the problem so it is easier to solve.\par
2 -> different instances of the same complex problem might have different difficulties. Often, we csn pre-process data of the instance to simplify the instance, like modify the constraints, fix some variables...\par
3 -> so we want to:\line -restrict domains of variables.\line -insert additional constraints (reducing the search space)\line -fixing variables\line -remove redundant constraints\line -detect early infeasibility\line Analyse sub-problems to understand if some of them can be pre-processed in this way. Most part of the work is done at the root node. This can also be seen as a special case of constraints programming: no obj function, but find feasible solution given the constraints. \par
4 -> bounds tightening.\par
5 -> interesting. We set the upper bound for each continuous variable.\par
6 -> We can also insert new constraints. Cover Inequalities: a cover is a subset of items st the sum of their weights exceed the capacity b. In the case of the KP, this tells that some items cannot all be put inside the knpascak, if their sum is greater than the capacity fo the Knapsack. We might have exponential number of covers and covering inequalties. They are all valid: a solution respects all of them. \par
7 -> how can we fix variables?\par
8 -> x^_ is the complement variable (oh it's written nvm). \par
11 -> i = row = connstraint. j = column = variable.\par
14 -> y_k is a non-binary variable. Underline means "minimum", lowest possible value of the variable. \par
15 -> like branching but before. I TENTATIVELY fix a variable. "What would happen if I fixed this variable?". We can propagate then some constraints. \par
16 -> k is an index, always.\par
21 -> this way of expressing the implication (system of two inequalities) allows automatic disaggregation of constraints. \par
23 -> the variables related in a clique are such that, when one is set to one, the others are set to 0. \par
\par
Lezione 4 - 10/10/2022\par
Linear programs with only binary variables.\par
2 -> it's called additive algorithm since we only require sums and subtractions.\par
3 -> it's a branch and bound.\par
4 -> Standard form of this particular problem, not of LP. \par
7-> If the right-hand side of the i-th constraint is positive for all constraints, then setting all free variables to 0 in this subproblem gives the best solution for S (new upper bound). \par
9 -> set to 0 the variables with positive coefficients, and to 1 those with negative coefficients. if there is a constraint such that t_i is not "negative enough", that means, it is larger than r_i(S), S is infeasible and the node can be discarded\par
11 -> all free variables that appear in the constraint can be fixed (they might not be all the free variables).\par
\par
Lezione 5 - 13/10/2022\par
Polyhedral combinatorics\par
What can we do with linear relaxation?\par
2 -> Basic fact: in discrete optimization problems, for a given problem, different constraints might define the same set of feasible solutions. So it makes sense to compare formulations and find the best one. \par
3 -> We'd like the formulation that gives us the best dual bound, so that we have the best b&b. The best formulation is the one that allows us to solve the MILP problem as if it was a LP problem. \par
4 -> blue: convex hull of the solutions, the smallest possible polyhedron that contains all the solutions. Its vertex are some integer solutions, and one of them is the best. For most discrete optimization problem, the ideal formulation is not known and difficult to compute. Might also imply a different number of facets (?). \par
5 -> some terminology. Given a discrete set of integer points x_1, ..., x_t, the convex hull is the set of real points such that each point can be optained as a convex combination of the initial points. Convex combination: we sum all these points, and each of them is multiplied b a suitable coefficient. Sum of coefficients = 1, all are non-negative. We'll use the convex hull definition when the points we use are discrete points that are possible solutions of the discrete optimization problem. The convex hull is contained within the valid formulation. \par
6 -> POLYHEDRAL COMBINATORICS STUDIES This stuff (sorry for capitals). Some discrete optimization problems, for them, we know the ideal formulation, but at the same time, for them, we don't need the ideal formulation. \par
7 -> How to select formulations and compare them? A common case is when we have constraints like this. i = index of items, j = index of bin. N = capacity of the bin, y_j = if the bin is used or not. (1) and (2) are different formulations for the same problem. \par
8 -> Summing the constraints for each item i in formulation (2) gives the first aggregate constraints. So, for each bin, we now have only one constraint. So, constraint (1) is a surrogate formulation of (2). So, (1) is a relaxation of (2). constraitns (2) imply constraints (1), but not the opposite. To demonstrate this, we have to find a point that satisfies (1) but not (2). Each bin is used 1/M of the time (N =100 and M = 5 i.e.). \par
28 -> There are algorithms that try to restrict the polyhedron. We insert constraints to tighten the polyhedron in the region close to the optimum. \line If we are able to continuously generate cutting points, sooner or later the algorithm will reach the optimal solution. Separation algorithm: the algo that generates new useful inequalities at each iteration. But if the problem (the original one) is difficult, the separation problem also is. This algorithm can be exact or heuristic. The tighter the constraint, the better. There is a fundamental result that states that the separation problem has the same complexity of the original problem.\par
19 -> we want the algorithm to generate at least a linear inequality such that all the points of the original problem respect the cosntraint, but the optimum of the relaxation (current one) does not. \par
20 -> cutting planes algorithms.\par
21 -> Validness and usefulness: both important for the new bounds introduced. Inequality 4 is faced defining: is one of the inequalities that is part of the ideal polyhedron. \par
\par
Lezione 6 - 17/10/2022\par
Oggi in italiano, non ci sono studenti internazionali (loooool)\par
2 -> Dobbiamo aggiungere vincoli per stringere il poliedro. Ci sono tecniche specifiche e generali per vari problemi. Vediamo quelle generali.\line Partiamo da problemi puramente interi, e moi come si estendono ai Misti-Interi.\line Con i puramente interi, partiamo da una solita funzione obiettivo di minimizzazione, variabili intere e vincoli di uguaglianza. Supponiamo anche che i coefficienti di A e b siano interi, non \'e8 fondamentale. Il rilassamento continuo del problema \'e8 lo stesso problema ma con variabili continue. Ora, cerchiamo di traslare i vincoli verso l'interno. Non modifichiamo insomma l'inclinazione del vincolo, cerchiamo solo di modificare il termine noto.\par
3 -> Terminologia: un iperpiano di supporto \'e8 tale per cui S \'e8 tutto dalla stessa parte, cio\'e8 cinteramente contenuto da uno degli iperspazi dell'iperpiano (d\'e9 io ci ho provato).\line Il vincolo rosso \'e8 un iperpiano di supporto perch\'e9 tiene il poliedro mnella parte valida e tocca un punto del poliedro. \par
4 -> Consideriamo adesso un insieme di iperpiani di supporto. h^T x = theta \'e8 un iperpiano, e h sono coefficienti interi. Di sicuro, il nostro poliedro \'e8 contenuto nell'intersezione di tutti questi iperpiani di supporto. \line Theta(P) \'e8 linsieme di tutti gli iperpiani di supporto con coefficienti interi a sinistra. Consideriamo ora una restrizione (contrario di rilassamento) di P, chiamato R(P), ovvero un insieme di punti nel continuo tali per cui i vincoli sono soddisfatti, ma in una versione dove la parte destra \'e8 rounded down. Di tutti gli iperpiani ristretti cos\'ec, ne prendiamo l'intersezione. Spostando gli iperpiani verso l'interno, rimane vero che tutti i punti interi del problema sono sempre soddisfatti dai nuovi vincoli. Questo \'e8 conseguenza del fatto che tutti i coefficienti di h sono interi. Questa procedura ha come difetto che, quando spostiamo il vincolo, non ci fermiamo quando troviamo il primo punto intero all'interno dell'insieme, ma ci fermiamo prima, perch\'e9 anche i punti FUORI dal poliedro bloccano questa operazione. Questi infatti sono i piani di Chv\'e0tal. Infatti, fino a questo punto storico, si pensava che questi piani fossero inutili nella pratica.\par
6 -> Si dimostra che questa restrizione di P \'e8 sempre un poliedro, essendo un'intersezione di iperpiani.\line So we can restrict the restrictions (an english student just arrived looooool). So, we generate new restrictions. Everytime, we generae a new plyhedron that is contained in the preceeding one. If we are lucky, we end up with the convex hull of X. It is proven that this Convex hull can be obtained after a finite number of steps. \line Polytope: poliedro ma chiuso in ogni direzione. Se \'e8 aperto anche solo in una direzione, \'e8 un poliedro ma non un politopo (? lol)\par
7 -> Gomory proposed a procedure. We take the linear relaxation of our problem and take the optimum. If the optimum is a fractional value, we can generate a new valid inequality. \line Z* can bre written as the sum of a constant term + the contirbution of non basic columns. In fact the non-basic columns give a contribution. N* represent the set of variables that are non basic at optimality. In the \{, we state that there is a matrix where there is a variable, for each row, that has 1 (?). Quello che ho scritto potrebbe essere sbagliato.\par
8 -> We generate inequalities by rounding. We select a variable which has not an integer value at optimality. That variable is basic in one of the constraints. \par
9 -> If we choose a good row for generating the gomory cut, we need a finite number of steps to reach the integer optimum.\par
10 -> WE want to extend this idea to MILP, but there are continuous variables too. We can extend, but is not trivial. \line Consider two valid inequalities, valid for two different polyhedron. Bot of them are of the kind <=. \par
15 -> f_i0 is between 0 and 1\par
16 -> k_ij is an integer we select arbitrarily. It remains the same because of the modulo. We can modify the coefficients and still be ok.\par
17 -> 11/10 perch\'e9 \'e8 come 1/10 + 1\par
22 -> f(b) is fractional part of b. The two inequalities are not the same. The second one cuts off some fractional solutions.\par
30 -> generate two SET of valid inequalities (point 2).\par
32 -> We had to assume that we had integer coefficients to reach the validiy of the Gomory cuts. If that is the case, the results for optimality with Gomory in the MILP cases are valid just as if we were in the ILP case. \par
\par
\par
\par
\par
\par
\par
\par
}
 