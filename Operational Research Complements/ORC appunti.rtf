{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1040{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.22000}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\qc\f0\fs22\lang16 Appunti di Operational Research Complements\par

\pard\sa200\sl276\slmult1\qj Lezione 1 - 29/09/2022\par
Recall of basic notions. \line The focus is on models and algorithms to solve complex probelms in mathematical programming. We want to solve NP-Hard problems. Operational Research mainly concentrated on models, and the only algo seen was the simplex algorithm for linear programming (Passacantando did also other things). here, we will also focus on how do design suitable algorithms for our problems. We'll FOCUS on algos for Mixed-Integer Linear Programming (some variables are discrete). Algos for Non-Linear Programming could be provided nontheless, though the Professor doesn't like them. \line This course is part of the Analytics and optimization stream: Combinatorial optimization, Heuristic algorithms for C.O. (efficiency not optimality) and Decision models and methods (more general, more objectives, two or more decision-makers, unvertainty in data). Also Simulation and Logistics are related.\line The exam consists of:\line -a project and an oral exam. The project requires to design, implement and test an algorithm to solve an NP-hard problem. 5 pages are enough for the report. The oral exam excludes the part of the project. Sometimes, a small written exam must be done before the development of the project, just to see if the student is going to waste its time or not.\line -another way to pass the exam is developing a paper about an algorithm and exposing it in a seminar. The seminar can basically replace the project.\line For CS people, the project might be better.\line One can also do a master degree thesis. The project can be the first chapter of the thesis.\par
Why attending and NOT attending this course? \line -We deal with optimization techniques. If we want to develop special skills for optimization algorithms. The optimization methods nowdays are at the base of many decision making systems (?) that are required nby many companies (consulting companies). "I'm a Operational Research Professional" doesn't make sense in Italy but it does abroad. \line -not attend this course if you are foreign and this is in english or you need credits with MAT label.\par
About the Operational Research Analyst: in the 2019...\line -n. of positions: 109700\line -Median annual salary: 84810 USD (+41% w.r.t. US median)\line -Forecasted growth 2018-2028: +26% (US: +5%)\line There is also the Certified Analytics professional that distinguish the TRUE people that knows the subject from those that just say they do. \par
Mathematical Programming.\line MP deals with optimization problems with no uncertainty, one objective and one decision-maker. We can further distingush this class in:\line -LP: linear constraints and objective, continuous variables.\line -MILP: as before but some variables are discrete.\line -Non-Linear Programming NLP: variables continuous, but constraints or objective are non linear.\line -MINLP: some variables are discrete and some constraints or the objective are not linear.\par
Let's recall some basics in Linear Programming.\line In LP we have variables with continuous domains, the constraints are linear equations and inequalities and the objective function is a linear function of the variables. (c^T x, not cx in the slides).\line We can always start from this formulation and replace equalities with inequalities (inequalities form). In this form, it's easy to give a geometric interpretation. In the geometrical interpretation of LP, equality defines an hyperplane and the inequality a halfspace. The constraints are conjuncted, and geometrically we have the intersection of halfspaces. This is a polyhedron. Polyhedron are convex sets.\line [[Following the slides]]\line If a problem is unbounded, the other is surely unfeasible, but the opposite is not true. If P is infeasible then also D might be unfeasible.\par
In discrete optimization probelms:\line primal bound = feasible but not optimal, dual bound = optimal but dot feasible\par
CPAIOR\par
Cplex, Gurobi, AMPL\par
\par
Lezione 2 - 03/10/2022\par
2a\par
5 -> you choose (a1, a2, ..., an)\par
9 -> For exploring in b&b, Depth-first (LIFO list) and Best-first (the list must be sorted) are better than Breadth-first (FIFO list). We also want to spend O(log(n)) for insertion and extraction. With linked list we would need O(1) for insertion and O(n) for extraction. To gain the first complesity, we can use a binary heap (tree). \par
10 -> branching produces big trees. Leaves are nodes that don't requrie further branching. \par
11 -> (Corollary 2) it he relaxation is the continuous one, it is true. (corollary 3) Here z^_ is just a random value, nothing to do with primal/dual bound. This corollary is the bounding of the b&b. \par
12 -> remember we are trying to minimize here, so the last line works. If the best solution of the relaxation is greater (worse) than the primal bound (feasible but worst) of the original problem, we close that node.\par
15 -> Producing leaves early is useful because it is a solution, and that is a feasible integer solution, that we can use as a primal bound. With DFS, we can heuristically find this feasible but sub-optimal solution basically. A recursive implementation of the B&B algorithm, the tree of activation records is managed by the OS. The algorithm becomes much faster in this way, rather than an explicit management of the Activation records. \par
2b\par
1-> We want the optimal permutation of items/tasks/objects/whatever.\par
4 -> the sum over all the j that follow i... so to compute those values we have to start from the last element of the permutation and backtrack to the start. \par
6 -> it's non-linear because of x_ij and alpha_j. \par
9 -> L = non fixed nodes (left). R = fixed nodes (right). \par
11 -> In the blu one, x_ij has already been fixed to 1.\par
12 -> LB_0 = Lower Bound 0\par
13 -> beta_i is a part of alpha_i. Can we estimate it?\par
17 -> at the root node, the gaps are computed (gaps = difference bw lower bound and optimum).\par
18 -> Largest at beginning, smallest at the end (it's an error).\par
19 -> p1 > p2 > p3 > p4\par
20 -> We want a measure of the effectiveness of the sorting policy. When do we find the optimal solution?\par
23 -> Size and time is similar, but the best solution is found before. It is useful if we want to truncate the search. Truncation is an important addition to branch and bound. We can set a timeout such that, when it is reached, the solver is stopped. The result is the best solution found so far. Usually one would like also to know the gap between the solution given and the lowe bound (dual bound). The smaller the gap, the better the solution (?). \par
26 -> Last line = number of optimal solutions found. \par
\par
Lezione 3 - 06/10/2022\par
Preprocessing -> things to do before b&b to the problem so it is easier to solve.\par
2 -> different instances of the same complex problem might have different difficulties. Often, we csn pre-process data of the instance to simplify the instance, like modify the constraints, fix some variables...\par
3 -> so we want to:\line -restrict domains of variables.\line -insert additional constraints (reducing the search space)\line -fixing variables\line -remove redundant constraints\line -detect early infeasibility\line Analyse sub-problems to understand if some of them can be pre-processed in this way. Most part of the work is done at the root node. This can also be seen as a special case of constraints programming: no obj function, but find feasible solution given the constraints. \par
4 -> bounds tightening.\par
5 -> interesting. We set the upper bound for each continuous variable.\par
6 -> We can also insert new constraints. Cover Inequalities: a cover is a subset of items st the sum of their weights exceed the capacity b. In the case of the KP, this tells that some items cannot all be put inside the knpascak, if their sum is greater than the capacity fo the Knapsack. We might have exponential number of covers and covering inequalties. They are all valid: a solution respects all of them. \par
7 -> how can we fix variables?\par
8 -> x^_ is the complement variable (oh it's written nvm). \par
11 -> i = row = connstraint. j = column = variable.\par
14 -> y_k is a non-binary variable. Underline means "minimum", lowest possible value of the variable. \par
15 -> like branching but before. I TENTATIVELY fix a variable. "What would happen if I fixed this variable?". We can propagate then some constraints. \par
16 -> k is an index, always.\par
21 -> this way of expressing the implication (system of two inequalities) allows automatic disaggregation of constraints. \par
23 -> the variables related in a clique are such that, when one is set to one, the others are set to 0. \par
\par
Lezione 4 - 10/10/2022\par
Linear programs with only binary variables.\par
2 -> it's called additive algorithm since we only require sums and subtractions.\par
3 -> it's a branch and bound.\par
4 -> Standard form of this particular problem, not of LP. \par
7-> If the right-hand side of the i-th constraint is positive for all constraints, then setting all free variables to 0 in this subproblem gives the best solution for S (new upper bound). \par
9 -> set to 0 the variables with positive coefficients, and to 1 those with negative coefficients. if there is a constraint such that t_i is not "negative enough", that means, it is larger than r_i(S), S is infeasible and the node can be discarded\par
11 -> all free variables that appear in the constraint can be fixed (they might not be all the free variables).\par
\par
Lezione 5 - 13/10/2022\par
Polyhedral combinatorics\par
What can we do with linear relaxation?\par
2 -> Basic fact: in discrete optimization problems, for a given problem, different constraints might define the same set of feasible solutions. So it makes sense to compare formulations and find the best one. \par
3 -> We'd like the formulation that gives us the best dual bound, so that we have the best b&b. The best formulation is the one that allows us to solve the MILP problem as if it was a LP problem. \par
4 -> blue: convex hull of the solutions, the smallest possible polyhedron that contains all the solutions. Its vertex are some integer solutions, and one of them is the best. For most discrete optimization problem, the ideal formulation is not known and difficult to compute. Might also imply a different number of facets (?). \par
5 -> some terminology. Given a discrete set of integer points x_1, ..., x_t, the convex hull is the set of real points such that each point can be optained as a convex combination of the initial points. Convex combination: we sum all these points, and each of them is multiplied b a suitable coefficient. Sum of coefficients = 1, all are non-negative. We'll use the convex hull definition when the points we use are discrete points that are possible solutions of the discrete optimization problem. The convex hull is contained within the valid formulation. \par
6 -> POLYHEDRAL COMBINATORICS STUDIES This stuff (sorry for capitals). Some discrete optimization problems, for them, we know the ideal formulation, but at the same time, for them, we don't need the ideal formulation. \par
7 -> How to select formulations and compare them? A common case is when we have constraints like this. i = index of items, j = index of bin. N = capacity of the bin, y_j = if the bin is used or not. (1) and (2) are different formulations for the same problem. \par
8 -> Summing the constraints for each item i in formulation (2) gives the first aggregate constraints. So, for each bin, we now have only one constraint. So, constraint (1) is a surrogate formulation of (2). So, (1) is a relaxation of (2). constraitns (2) imply constraints (1), but not the opposite. To demonstrate this, we have to find a point that satisfies (1) but not (2). Each bin is used 1/M of the time (N =100 and M = 5 i.e.). \par
28 -> There are algorithms that try to restrict the polyhedron. We insert constraints to tighten the polyhedron in the region close to the optimum. \line If we are able to continuously generate cutting points, sooner or later the algorithm will reach the optimal solution. Separation algorithm: the algo that generates new useful inequalities at each iteration. But if the problem (the original one) is difficult, the separation problem also is. This algorithm can be exact or heuristic. The tighter the constraint, the better. There is a fundamental result that states that the separation problem has the same complexity of the original problem.\par
19 -> we want the algorithm to generate at least a linear inequality such that all the points of the original problem respect the cosntraint, but the optimum of the relaxation (current one) does not. \par
20 -> cutting planes algorithms.\par
21 -> Validness and usefulness: both important for the new bounds introduced. Inequality 4 is faced defining: is one of the inequalities that is part of the ideal polyhedron. \par
\par
Lezione 6 - 17/10/2022\par
Oggi in italiano, non ci sono studenti internazionali (loooool)\par
2 -> Dobbiamo aggiungere vincoli per stringere il poliedro. Ci sono tecniche specifiche e generali per vari problemi. Vediamo quelle generali.\line Partiamo da problemi puramente interi, e moi come si estendono ai Misti-Interi.\line Con i puramente interi, partiamo da una solita funzione obiettivo di minimizzazione, variabili intere e vincoli di uguaglianza. Supponiamo anche che i coefficienti di A e b siano interi, non \'e8 fondamentale. Il rilassamento continuo del problema \'e8 lo stesso problema ma con variabili continue. Ora, cerchiamo di traslare i vincoli verso l'interno. Non modifichiamo insomma l'inclinazione del vincolo, cerchiamo solo di modificare il termine noto.\par
3 -> Terminologia: un iperpiano di supporto \'e8 tale per cui S \'e8 tutto dalla stessa parte, cio\'e8 cinteramente contenuto da uno degli iperspazi dell'iperpiano (d\'e9 io ci ho provato).\line Il vincolo rosso \'e8 un iperpiano di supporto perch\'e9 tiene il poliedro mnella parte valida e tocca un punto del poliedro. \par
4 -> Consideriamo adesso un insieme di iperpiani di supporto. h^T x = theta \'e8 un iperpiano, e h sono coefficienti interi. Di sicuro, il nostro poliedro \'e8 contenuto nell'intersezione di tutti questi iperpiani di supporto. \line Theta(P) \'e8 linsieme di tutti gli iperpiani di supporto con coefficienti interi a sinistra. Consideriamo ora una restrizione (contrario di rilassamento) di P, chiamato R(P), ovvero un insieme di punti nel continuo tali per cui i vincoli sono soddisfatti, ma in una versione dove la parte destra \'e8 rounded down. Di tutti gli iperpiani ristretti cos\'ec, ne prendiamo l'intersezione. Spostando gli iperpiani verso l'interno, rimane vero che tutti i punti interi del problema sono sempre soddisfatti dai nuovi vincoli. Questo \'e8 conseguenza del fatto che tutti i coefficienti di h sono interi. Questa procedura ha come difetto che, quando spostiamo il vincolo, non ci fermiamo quando troviamo il primo punto intero all'interno dell'insieme, ma ci fermiamo prima, perch\'e9 anche i punti FUORI dal poliedro bloccano questa operazione. Questi infatti sono i piani di Chv\'e0tal. Infatti, fino a questo punto storico, si pensava che questi piani fossero inutili nella pratica.\par
6 -> Si dimostra che questa restrizione di P \'e8 sempre un poliedro, essendo un'intersezione di iperpiani.\line So we can restrict the restrictions (an english student just arrived looooool). So, we generate new restrictions. Everytime, we generae a new plyhedron that is contained in the preceeding one. If we are lucky, we end up with the convex hull of X. It is proven that this Convex hull can be obtained after a finite number of steps. \line Polytope: poliedro ma chiuso in ogni direzione. Se \'e8 aperto anche solo in una direzione, \'e8 un poliedro ma non un politopo (? lol)\par
7 -> Gomory proposed a procedure. We take the linear relaxation of our problem and take the optimum. If the optimum is a fractional value, we can generate a new valid inequality. \line Z* can bre written as the sum of a constant term + the contirbution of non basic columns. In fact the non-basic columns give a contribution. N* represent the set of variables that are non basic at optimality. In the \{, we state that there is a matrix where there is a variable, for each row, that has 1 (?). Quello che ho scritto potrebbe essere sbagliato.\par
8 -> We generate inequalities by rounding. We select a variable which has not an integer value at optimality. That variable is basic in one of the constraints. \par
9 -> If we choose a good row for generating the gomory cut, we need a finite number of steps to reach the integer optimum.\par
10 -> WE want to extend this idea to MILP, but there are continuous variables too. We can extend, but is not trivial. \line Consider two valid inequalities, valid for two different polyhedron. Bot of them are of the kind <=. \par
15 -> f_i0 is between 0 and 1\par
16 -> k_ij is an integer we select arbitrarily. It remains the same because of the modulo. We can modify the coefficients and still be ok.\par
17 -> 11/10 perch\'e9 \'e8 come 1/10 + 1\par
22 -> f(b) is fractional part of b. The two inequalities are not the same. The second one cuts off some fractional solutions.\par
30 -> generate two SET of valid inequalities (point 2).\par
32 -> We had to assume that we had integer coefficients to reach the validiy of the Gomory cuts. If that is the case, the results for optimality with Gomory in the MILP cases are valid just as if we were in the ILP case. \par
\par
Lezione 7 - 20/10/2022\par
Special cutting planes for ILP and MILP\par
1 -> Cover inequalities for 0-1 programming\line Cover: subset of items such that their total consumption of the capacity is larger than the capacity of the knapsack. It is also minimal iif it does not conain any other cover.\par
2 -> For each cover, we can associate a valid cover inequality. \par
5 -> We can obtain valid inequalities for our polyhedron by using a technique called lifting. The cover inequalities are commonly used by MILP solvers to strenghten formulations. It has been proven that minimal covers and lifting allow us to obtain all the non-trivial facets of any 0-1 programming poytope with positive coefficients. \par
6 -> The excess in the cover (lambda c) is always minimal if it is a minimal cover. \par
8 -> IMagine a flow network, with nodes receiving flows from arcs and sending flows to other nodes with other arcs. \par
16 -> full dimensional: we cannot elimitate any variabile, there are no redundant variables.\line Also, all non-trivial facets have positive coefficients. In 2D, this means that if we are maximixing, then the constraints are oriented going in the opposite direction of the objective. \par
26 -> m, not n, in the last time "n" is reported.\par
general concept of antiewb inequalities: a possible question at the exam, but not all the details.\par
\par
Lezione 8 - 24/10/2022\par
Lifting\par
2 -> Extension: another technique, but weaker than lifting\par
4 -> the sum is extended to all the elements of the extended cover, not just the cover itself. We include the cover and all other elements s.t. their coefficient is greater thatn any coefficient of any element in the cover. \par
5 -> Sccume variables are binary\par
6 -> We want to lift the variables that don't appear in the cover\par
7 -> N = set of all the variables.\par
8 -> phi depends on the variables already in M. U is the coefficeint a_k (...can be interpreted as). pi_k is phi_M(a_k)\par
9 -> Lifting variables in different orders yields different results.The earlier a variable is lifted, the larger is its coefficient. He was a PHD student in the Netherlands :)\line So, there is a combinatorial explosion of the possible sequences in which the variables can be lifted\par
12 -> The definition is an intermediate step for finding a PSI function that we want.\par
19 -> 18, not 19.\par
Simultaneous lifting is also facet defining? It is guaranteed to be valid, but... the teacher will check. \par
\par
Lezione 9 - 27/10/2022\par
Branch and cut.\par
2 -> Gomory cuts were not common at the beginning (reasons on the slides). But in case of mixed binary linear programming, we can generate gomory cuts for the B&B tree. \par
Slides successive\par
6 -> Interior Point Algorithms - instead of simplex, because it behaves better (i mean, the Interior Point Algorithm) when there is degeneracy.\par
7 -> fixing the variable out of the basis\par
8 -> Instead of using all the variables, you just use a subset: the most promising variables. They are more likely to be basic in the optimal solution of the problem. \par
9 -> Intensification and Diversification are Exploitation and Exploration. \par
14 -> delta(i) is the set of edges incident to node i. \par
15 -> delta(S) = edges with ONE endpoint in S and the OTHER ONE outside S.\line E(S) = edges with both endpoints in S. \line They are two complementary definitions: one uses cuts, one uses cycles. \line In real formulation, you either use (2) or (3), but you have to generate an exponential number of constraints (one for each S possible set).\par
\par
Lezione 10 - 03/11/2022\par
Let's go on with the TSP and ATSP\par
17 -> No subtour elimination constraints.\par
18 -> We start with a negative result. The constraints seen are not enough to fully describe the TSP polyhedron. The fractional solution that can be found is not acceptable (is fractional), but satisfies all the constraints. \par
19 -> 2-matching inequality. Feasible for all integer solution, but not for fractional solutions.\par
21 -> Comb (pettine) inequalities. \par
22 -> Some variables might appear twice.\par
24 -> With all this study, we can solve a lot of complex problems of the TSP in very small time. \par
27 -> delta(S)\\T = all the arcs going out of S BUT the red ones. Polynomial time separation is possible in some cases.\par
31 -> It is possible to transform a ATSP into a TSP, but with the cost that we need thrice the nodes.\par
\par
Lezione 11 -> 07/11/2022\par
Lagrangian relaxation. Before, we saw linear relaxation and its consequences.\par
2 -> We'll use LR in a branch and bound algorithm. \par
3 -> negative -> the solution complies the contraint. Positive -> the constraint is violated (for the red b - Ax). The LR says: take the obj function. If a constraint is violated, its solution is penalized. Otherwise, it is rewarded. \par
5 -> You have an equality constraint? you can turn them into inequality constraints. \line Ax >= b => -Ax <= -b, and lambda = lambda' - lambda^2 (?)\par
9 -> pink line = slope of obj function.\line Imagine that the red constraint doesn't allow us to solve easily the problem. But if we remove it, it is easier. IMAGINE, ok? It's to show the geometric meaning of the relaxation. \par
10 -> max Z_lr is the lower (so upper in this case) bound, we want it as small as possible. So, we need lambda as small as possible. Extreme cases: 0 and +inf. With a value in the middle, we can change the slope of the obj function. It's like taking the convex combination of two objectives. In this PARTICULAR example, the optimal solution of the relaxation is the same as the one of the original problem, something that usually doesn't happen.\par
7 -> Lagrangean dual problem: what is the best value for lambda? (We assume that here the problem is started from a minimization problem, which is the opposite wrt the problem we saw earlier). Z* ld is the best possible dual bound (here maximum lower bound we can get from lagrangean relaxation. In the example before, the best lower bound was the lowes upper bound). \par
8 ->With LR, we can't say that the optimum of LR is also the optimal for P. To be optimal, we require both feasibility and complementarity. Complementary is implied by feasibility, if we had equality constraints (???).\par
11 -> fix lambda\par
13 -> fix x. We have an exponential number of lines. This is a linear programming problem with an exponential number of constraints. Can we use simplex? No, the input is too large. one constraint for each feasible solution of the og problem. From an algorithmic viewpoint, we need different methods.\par
14 -> 1/9 here is the best value for lambda. In x*, the value fo the upper bound is the value of the lagrangean relazation function computed in aaa (x* is a value between the two extreme points). In x*, the violation is 0. \par
Covexification: when we can solve a subproblem of the original problem by convexifiyng its feasible region. \par
In general, the optimal value of the lagrangean dual is the max of the original function subkject to the relax constraint and the constraints defining the convex hull of the problem. With lagrangean relaxation, we are convexifying something yes and something not.\par
15 -> how good is LR?\line we have positive and negative results. The LR is between the best case and linear relaxation.\ul\par
\ulnone 17 -> The middle is "Lagrangead dual" (i think), while the one on the right is "Linear Problem".\par
6 -> He's talking about this slide (Observations)\par
\par
lezione 12 -> 14/11/2022\par
The next lecture will be skipped :(\par
22 -> There always is a trade off between things. In this case, how good is a lagrangean relaxation? Because it requires to solve the lagrangean subproblem and the dual problem. Is this good? Depends. How many constraints do we relax? How difficult it is to solve the relaxed problem? It might be worth to use the algrangean relaxation if we have a large problem, such that computing the linear relaxation would be efficient (because the simplex would require too much time to solve it). \par
24 -> This is a binary knapsack problem. Can we solve it easily? Actually yes. \par
25 -> Another kind of relaxation. This time we relax the capacity constraints.\par
26 -> What if we relax everything? \par
27 -> See slide 13 for visual reference\par
29 -> The step must be large enough, so that we can cover every instance from the initial point to the alst one. It's a divergent series (?). In the end, the steps must be small: they allow us to make small adjustments to reach the optimum. \par
\par
Lezione 14 -> 24/11/2022\par
MMCF\par
Multi-commodity Min Cost Flow prblem.\line We have a graph, a set of commodities K...\par
Examples of Lagrangean Relaxation with AMPL.\par
.mod, .dat and .run files.\par
Alternating between feasibility and optimality.\par
\par
Now, Held and Karp.\par
2 -> Their lower bound for the TSP is very thight.\par
\par
\par
}
 